---
title: 'HDP读后感'
date: 2025-01-01
permalink: /posts/2025/01/blog-post-1/
tags:
  - book
---
High-Dimensional Probability (2018) - Vershynin 是一本介绍高维变量 concentration inequalities 的书，从 random variable, random vector, random matrix 到 random process，涵盖的内容很广。感激滕佳烨老师的[视频](http://www.tengjiaye.com/HDP.html)！

concentration 是 expectation 形式（弱）或是 tail probability 形式（强）。

## Random Variable

Random variable 讨论的是 concentration of summation。
- 通过加 sub-gaussian/sub-exponential 的假设，得到 general Hoeffding/Bernstein 不等式
- 应用: degree of random graph 的 bound。

## Random Vector

- Concentration of norm (dimension趋于无穷而非summation) 
  - 对于 component-wise independent and component-wise sub-gaussian 的 random vector, 可以给出 norm 的双边 bound (Theorem 3.1.1)。
  - 然后我们定义了 vector 的 sub-gaussian。对于 component-wise independent 的 sub-gaussian vector，只能给出单边的bound (Exercise 6.3.5). 
  - 应用: Grothendieck 不等式和 SDP relaxation. 在 optimization 课上，我们已经见过了 quadratic optimization \\(\min_{x} x^\top A x\\) problem （如 max-cut problem） relax 为 SDP 的求解思想，这里我们进一步考虑 integer quadratic optimization: 要求 x 的每个元素都是 1,-1. 通过Grothendieck 不等式，我们可以把 integer relax 为 continuous value 并给出 relaxation bound.
- Concentration of a \\(f(X)-E[f(X)]\\) (without independence, generalize the norm)
  - 需要假设 Lipschitz continuous of f and specific distribution of X。
- Concentration of quadratic form
  - Hanson-Wright inequality 给出了 concentration；证明用到 decoupling 的思想，即引入随机变量的一个副本 X'。延申：Rademacher complexity 和 VC dimension 都会用到的 symmetrization，其证明也是使用了同样的思想，引入随机变量的一个副本 X'。

## Random Matrix

- Concentration of F-norm and operator norm
  - 当 component-wise independent and sub-gaussian 时，有结论 Theorem 4.4.5；
  - 当 row-wise independent and sub-gaussian 时，有结论 Theorem 4.6.1；
  - 对于无 sub-gaussian 时，只有 expectation 形式的结论。
  - 证明的思路是 disceretize as \\(\epsilon\\)-net, thus involves covering number. 
  - 应用：community detection in network.

- Concentration of norm of summation
  - Matrix Hoeffding inequality and matrix Bernstein inequality
  - 应用：J-L Lemma 给出了降维的保距性。 covariance estimation。

## Random Process

Consider \\( E[\sup_{t\in T} X_t] \\)
- Gaussian process
  - Sudakov inequality: lower bound
- General process: with sub-gaussian increment
  - Dudley's inequality. Proof: chaining 是加强版 \\(\epsilon\\)-net. 改进：generic chaining bound，use adaptive \\(\epsilon_k\\)
  - 应用：empirical process via VC dimension

## Others

Vector recovery, matrix recovery; Matrix deviation inequality, Dvoretzky-Milman Theorem.